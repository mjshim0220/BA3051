
## Load necessary libraries, clear the environment

Run this before starting. You should have all these libraries and executables already installed. 

```{r}
library(devtools)
library(resampledata)
library(tidyverse)
library(mlba)
library(fastDummies)
library(caret)
library(texreg)
library(highlight)
library(readxl)
library(broom)
library(forecast)
library(zoo)

```

## 1.0 Shoe Size Predictions

### 1.1 Load the shoe size data

First you will have to save the .csv file to your local computer (ideally the same place as the previous file). You should have already update the working directory to this location above. A common convention is to indicate that the data is a "data frame" (as opposed to a list or other R object) by including the suffix ".df" after the name (not required but useful syntax).

```{r}
shoe_size.df <- read.csv("/Users/mj/IN PROGRESS Research/R projects/BA3051_git/data/Shoe_sizes.csv", header=T)
#view(shoe_size.df)

#read Excel file into R with windows system
#shoe_size2.df <-read_excel("Regression_data.xlsx", sheet = 1)


```

### 1.2 Understand the data

What variables do we have, and what values do they take? The outcome variable of interest here is "Shoe_Size". Age and Weight are self explanatory. Sex is an indicator (dummy variable) that equals 1 for Male and 0 for female. Right now it's stored as an integer (meaning that it could technically take on any numerical value, similar to age or weight). Let's create a factor version of this variable so we can later compare the use of a dummy integer vs. dummy factor. We do this using the `mutate()` function. We indicate what the existing levels are, and what labels we want to assign to each of those labels for the new factor variable.

```{r}
str(shoe_size.df)

shoe_size.df <- shoe_size.df %>% 
  mutate(
    Sex_factor = factor(Sex, levels=c(0,1), labels = c("female","male"))
  )

str(shoe_size.df)
```

### 1.3 Estimate a model (regression) to predict shoe size

We will first estimate a model to predict shoe size using four variables (age, weight, sex, and IQ). We do so using the `lm()` function (short for linear model), listing the dependent variable first followed by a tilde (\~) and then the explanatory variables of interest, separated by a plus sign (+). We will save this regression to an object named "shoesize_lm.1". We can see a regression summary using the `summary()` function, but a better formatted one using `screenreg()`. We can also see additional details (such as R-squared) using the `glance()` function.

***Questions:*** What are the effects of each of these four variables?

```{r}
shoesize_lm.1 <- lm(Shoe_Size~Age+Weight+Sex+IQ_Score, data = shoe_size.df) 

summary(shoesize_lm.1) #results of model
screenreg(shoesize_lm.1) #better formatted results
glance(shoesize_lm.1) #additional details from model


shoesize_lm.2 <- lm(Shoe_Size~Age+Weight+Sex_factor+IQ_Score, data = shoe_size.df) 
summary(shoesize_lm.2)

#View models side by side
screenreg(list(shoesize_lm.1,shoesize_lm.2), digits=3)


```

### 1.4 Adjust the model

We notice that one of the variables (IQ) is insignificant. Does this make economic sense? Probably. Let's try another model that excludes this insignificant control.

***Questions:*** What happens to the model summary statistics when IQ is dropped?

```{r}

#Note that IQ_Score is insignificant. Let's try removing that variable
shoesize_lm.3 = lm(Shoe_Size~Age+Weight+Sex, data = shoe_size.df) 


#View models side by side
screenreg(list(shoesize_lm.2,shoesize_lm.3), digits=3)
```

### 1.5 Bring in predicted and residual outcomes

The main purpose of this model is to generate predicted shoe sizes for a new individual. Let's first evaluate the model and compare the actual outcome, the predicted, and the residuals. We will first manually extract the predicted shoeside and residual show size (difference between predicted and actual) from our second model and merge them back into our original dataframe (shoe_size.df). This way we can see them all side by side. We will also manually calculate the residuals as the difference between the actual and predicted outcomes.

We will next look at summary statistics for these variables. By construction, the mean residual should be zero for the model when applied to the training data set.

```{r}

#Add the residuals and predicted values to the shoe_size data frame 
#Manually verify that residual is the difference between the actual and predicted shoe size

shoe_size.df$residuals <- residuals(shoesize_lm.3) 
shoe_size.df$predicted_size <- fitted.values(shoesize_lm.3)
shoe_size.df$residuals_manual = shoe_size.df$Shoe_Size - shoe_size.df$predicted_size 

summary(shoe_size.df$residuals) #summary stastics for a single variable
summary(shoe_size.df[c("Shoe_Size","predicted_size","residuals")]) #summary statistics for a subset of variables


```

As an alternative, we can use the function `augment()` to tidy up our model and do this all in one step. This will store the fitted values as the variable ".fitted" and the residuals as ".resid", which is how they are originally stored in the model. We can use the `summary()` function and list the specific list of variables we want to see.

```{r}
shoe_model.df <- augment(shoesize_lm.3)

summary(shoe_model.df[c("Shoe_Size",".fitted",".resid")])

```

### 1.6 Compare predicted and residuals

We can also view the predicted values and residuals directly from the model. A good practice is to plot these using a scatterplot. We accomplish this using the `geom_point()` function (see Visualization lecture). We will put the fitted values (the predictions from our model, stored as the variable ".fitted") on the x-axis and the residual values (observed - predicted, stored as the variable ".resid") on the y-axis.

In a well-behaved linear regression model, you would ideally see a random scatter of points around the horizontal line at y = 0, indicating that the residuals are randomly distributed and have constant variance. Patterns or trends in the plot might suggest violations of assumptions, such as heteroscedasticity or non-linearity. We can use either the model itself (shoesize_lm.3) or the data we extracted from the model using the augment() function (shoe_model.df) or the data frame where we added the residuals.

```{r}
#Option #1
ggplot(shoesize_lm.3,aes(x=.fitted, y=.resid)) +
  theme_classic()+
  geom_point()+
  geom_hline(yintercept=0) +
  labs(y="Residuals", x="Fitted Values", title = "Predicted Values")

#Option #2
ggplot(shoe_model.df,aes(x=.fitted, y=.resid)) +
  theme_classic()+
  geom_point()+
  geom_hline(yintercept=0) +
  labs(y="Residuals", x="Fitted Values", title = "Predicted Values")

ggplot(shoe_size.df,aes(x=predicted_size, y=residuals)) +
  theme_classic()+
  geom_point()+
  geom_hline(yintercept=0) +
  labs(y="Residuals", x="Fitted Values", title = "Predicted Values")
```

### 1.7 Generate predicted shoe sizes for a new observation

Our previous code gave us the predicted outcome for every individual in our data set, and we could compare these predictions to the actual shoe size to get a sense of how accurate our model is. But what if someone new comes along? A model is best applied for filling in missing data--generating a prediction for something you don't yet know.

Using the first model, let's generate a prediction for a 15 year-old female who weighs 100 lbs and has an IQ score of 115. We provide numeric values for age, weight, and IQ_Score, as this is how the data was used. For sex_factor, because it's a factor variable we need to enter the label we assigned for females as a string. Next, generate a prediction using the second model. Note we have to exclude IQ_score, as this wasn't used to estimate the third model.

```{r}

predict(shoesize_lm.2, list(Age = 15, Weight = 100, Sex_factor = "female", IQ_Score = 115))


predict(shoesize_lm.3, list(Age = 15, Weight = 100, Sex = 0))
```

## 2.0 Vehicle Sales & Gas Prices

### 2.1 Load the vehicle sales data

First you will have to save the .csv file to your local computer (ideally the same place as the previous file). You should have already update the working directory to this location above. A common convention is to indicate that the data is a "data frame" (as opposed to a list or other R object) by including the suffix ".df" after the name (not required but useful syntax).

```{r}
#rm(list=ls()) #clears the entire workspace for a fresh start

salesdata.df = read.csv("/Users/mj/IN PROGRESS Research/R projects/BA3051_git/data/Car_Sales-1.csv")
#view(salesdata.df)

#read Excel file into R with windows system
#salesdata2.df <-read_excel("Regression_data.xlsx", sheet = 2)

```

### 2.2 Simple Linear Regressions (one explanatory variable)

#### 2.2.1 Estimate the model

We will run our first linear model by regression sales_vehicle on price_gas. To get more detailed model statistics, we will need to save the model as an object (which we will name sales_lm.1). We can see these statistics using the `summary()` function. We can also view a nicely formatted table using the `screenreg()` function.

***Questions***: What is the association between gas prices and sales volume? Does it make sense? What is the overall explanatory power of this model?

```{r}

lm(Sales_vehicle~price_gas, data = salesdata.df) 

sales_lm.1 <- lm(Sales_vehicle~price_gas, data = salesdata.df)

model_data <- augment(sales_lm.1)
summary(sales_lm.1) 

screenreg(sales_lm.1, digits = 3)

```

#### 2.2.2 Evaluate residuals

Good practice is to evaluate the residuals. We can next create a scatterplot, putting the predicted or fitted values on the x-axis (what our model predicts for each inputted x-variable). On the y-axis we have the residuals, meaning the difference between these predictions and the observed values).

In a well-behaved linear regression model, you would ideally see a random scatter of points around the horizontal line at y = 0, indicating that the residuals are randomly distributed and have constant variance. Patterns or trends in the plot might suggest violations of assumptions, such as heteroscedasticity or non-linearity.

```{r}

ggplot(sales_lm.1,aes(x=.fitted, y=.resid)) +
  theme_classic()+
  geom_point()+
  geom_hline(yintercept = 0) +
  labs(y="Residuals", x="Fitted Values", title = "predicted values")
```

### 2.4 Multiple Linear Regressions using two explanatory variables

We will next augment our model to also include income. We will save this model as the object sales_lm.2. We can show our two models side by side.

***Questions***: how do the two models differ? How are they similar? What is the difference in explanatory power?

```{r}
sales_lm.2 <- lm(Sales_vehicle~price_gas+income, data = salesdata.df) 

screenreg(sales_lm.2)

#Display both models side by side 
screenreg(list(sales_lm.1,sales_lm.2), digits=3)

```

**Interpretation**: after adding income as an additional explanatory variable, we see that the association with gas prices is now flipped (negative). So higher gas prices lead to lower vehicle sales, while higher incomes lead to more vehicle sales. This intuitively makes sense. We also can explain much more of the variation in vehicle sales.

## 3.0 College Admissions

First you will have to save the .csv file to your local computer. You will then need to modify the file path to where you have this file saved. Note that you must use the forward slash (/) and not the backslash (which is reserved for special coding purposes in R).

### 3.1 Load the data

First you will have to save the .csv file to your local computer (ideally the same place as the previous file). You should have already update the working directory to this location above. A common convention is to indicate that the data is a "data frame" (as opposed to a list or other R object) by including the suffix ".df" after the name (not required but useful syntax).

```{r}
#rm(list=ls()) #clears the entire workspace for a fresh start

admission_data.df = read.csv("/Users/mj/IN PROGRESS Research/R projects/BA3051_git/data/College_Admissions-1.csv", header = T)

```

### 3.2 Understand the data

#### 3.2.1 How are the variables formatted?

Our outcome variable of interest is "SAT_ACT"" which is stored as an integer. Notice that for the string variables (e.g., Gender, College, Admitted, Enrolled), we get pretty useless information from the `summary()` function such as the length of the variable. Other variables also have misleading summary statistics, such as applicant. The summary statistics for White and Asian are actually useful, and tell use the percentage of each in the sample. We will improve on this below by converting these variables to factor variables.

```{r}
str(admission_data.df)

summary(admission_data.df)

```

#### 3.2.2 Create factor variables

We see several variables that ideally should be treated as factor variables. The data includes a string variable "Gender" which is formatted as a string (chr) and stored as either "F" or "M". We also have several variables stored as either integers or strings which should also be ideally converted to factor variables. This will help with interpreting the model as well as in created dummy variables below.

```{r}
admission_data.df <- admission_data.df %>% 
  mutate

admission_data.df <- admission_data.df %>%
  mutate(
    Applicant = as.factor(Applicant),
    Gender = as.factor(Gender),
    White = as.factor(White),
    Asian = as.factor(Asian),
    College = as.factor(College),
    Admitted = as.factor(Admitted),
    Enrolled = as.factor(Enrolled)
  )
str(admission_data.df)
```

#### 3.2.3\* Create dummy variables from factor variables

***Advanced:*** R handles factor variables quite well in regressions. You also have the option though of creating dummy variables from a factor variable and manually listing n-1 of these dummies in your model.

```{r}
admission_data_dummies.df <- admission_data.df %>% 
  dummy_cols(select_columns=c('College'),
  remove_selected_columns=FALSE,
  remove_first_dummy=FALSE)

str(admission_data_dummies.df)
```

#### 3.2.4 Summary statistics

With the appropriate variables correctly converted as factor variables, our summary statistics are now much more useful. We can also see a breakdown of any of these variables using the `table()` function and `prop.table()` function.

```{r}
summary(admission_data.df)

table(admission_data.df$Gender)
round(prop.table(table(admission_data.df$Gender))*100, digits=1)

table(admission_data.df$College)
round(prop.table(table(admission_data.df$College))*100, digits=1)

```

### 3.3 Estimate several models (regressions) to predict SAT score

Our first baseline model includes three explanatory variables (high school GPA and number of years parent's went to school). The second model adds the Gender dummy. The third model includes controls for ethnicity. We have two indicators: 'White' and 'Asian'. Note that there are instances where neither equals 1, and which correspond to the benchmark ethnicity (African American). The fourth model adds controls for college. The baseline category again is the omitted one (in this case `Arts & Letters`).

**Question**: what is captured by the intercept in each model?

```{r}
#Model 1: Baseline model
SAT_lm.1 = lm(SAT_ACT ~ HSGPA + Edu_Parent1 + Edu_Parent2 , data = admission_data.df)
screenreg(SAT_lm.1, digits = 2)

#Model 2: Dummy for gender
SAT_lm.2 = lm(SAT_ACT ~ HSGPA + Edu_Parent1 + Edu_Parent2 + Gender , data = admission_data.df)
screenreg(list(SAT_lm.1,SAT_lm.2), digits = 2)

#Model 3: Dummies for ethnicity
SAT_lm.3 = lm(SAT_ACT ~ HSGPA + Edu_Parent1 + Edu_Parent2 + White + Asian, data = admission_data.df)
screenreg(list(SAT_lm.1,SAT_lm.2,SAT_lm.3), digits = 2)


#Model 4: Dummies for college
SAT_lm.4 = lm(SAT_ACT ~ HSGPA + Edu_Parent1 + Edu_Parent2 + College, data = admission_data.df)
screenreg(list(SAT_lm.1,SAT_lm.2,SAT_lm.3,SAT_lm.4), digits = 2)

```

### 3.4 Master model

We can put all these together now in a master model with all of our factor/dummy variables.

**Question**: what is captured by the intercept now in model 5?

```{r}
#Model #5: 
SAT_lm.5 = lm(SAT_ACT ~ HSGPA + Edu_Parent1 + Edu_Parent2 + Gender + White + Asian + College, data = admission_data.df)

screenreg(list(SAT_lm.1,SAT_lm.2,SAT_lm.3,SAT_lm.4,SAT_lm.5),digits=2)

```

## 4.0 Real Estate Values

We will explore the practice of partitioning the data in training and holdout samples in this exercise.

### 4.1 Create partitions

```{r}
#rm(list=ls()) #clears the entire workspace for a fresh start

housing.df <- mlba::WestRoxbury %>%
  mutate(REMODEL=factor(REMODEL)) %>% 
  unique()

# use set.seed() to get the same partitions when re-running the R code.
set.seed(1)

## partitioning into training (60%) and holdout (40%)
# randomly sample 60% of the row IDs for training; the remaining 40% serve as holdout
train.rows <- sample(rownames(housing.df), nrow(housing.df)*0.6) # collect all the columns with training row ID into training set:
train.df <- housing.df[train.rows, ]

# assign row IDs that are not already in the training set, into holdout
holdout.rows <- setdiff(rownames(housing.df), train.rows)
holdout.df <- housing.df[holdout.rows, ]



```

### **4.2 Using caret for data partition**

```{r}

### Alternative partition approach that preserves the distribution of the outcome variable of interest using caret

set.seed(1)
idx <- caret::createDataPartition(housing.df$TOTAL.VALUE, p=0.6, list=FALSE)
train.df <- housing.df[idx, ]
holdout.df <- housing.df[-idx, ]
```

### 4.3 Regression

#### 4.4 Prepare the data

```{r}
housing.df <- mlba::WestRoxbury %>%
  # remove rows with missing values
  drop_na() %>%
  #keep only unique records
  unique() %>% 
  # remove column TAX
  select(-TAX) %>%
  # make REMODEL a factor and convert to dummy variables
  mutate(REMODEL=factor(REMODEL)) %>%
  dummy_cols(select_columns=c('REMODEL'),
             remove_selected_columns=TRUE, remove_first_dummy=TRUE)

set.seed(1)
idx <- caret::createDataPartition(housing.df$TOTAL.VALUE, p=0.6, list=FALSE)
train.df <- housing.df[idx, ]
holdout.df <- housing.df[-idx, ]


```

```{r}
#Run the regression with TOTAL.VALUE as dependent variable

reg <- lm(TOTAL.VALUE ~ KITCHEN + BEDROOMS + FLOORS, data=train.df)  #if you want to manually select the control variables

reg <- lm(TOTAL.VALUE ~ ., data=train.df) #Incluode all other variables in the data frame, using the train.df dataframe

summary(reg) #Look at a summary of the regression model
screenreg(reg, digits = 3)

train.res <- data.frame(actual=train.df$TOTAL.VALUE, 
                        predicted=reg$fitted.values,
                        residuals=reg$residuals)

head(train.res)

#Generate predictions for holdout data
pred <- predict(reg, newdata=holdout.df) #predict outcomes using the holdout dataframe

holdout.res <- data.frame(actual=holdout.df$TOTAL.VALUE, predicted=pred,   #Combine the actual values, the predicted values, and the difference (residuals)
                          residuals=holdout.df$TOTAL.VALUE - pred)

head(holdout.res)
```

#### 4.5 Evaluation metrics for regression

```{r}

# compute metrics on training set
data.frame(
    ME = round(mean(train.res$residuals), 5),
    #Root Mean Squared Error (RMSE) penalizes large errors more heavily than smaller ones
    RMSE = RMSE(pred=train.res$predicted, obs=train.res$actual),
    #MAE Mean Absolute Error 
    MAE = MAE(pred=train.res$predicted, obs=train.res$actual)
)

# compute metrics on holdout set
data.frame(
    ME = round(mean(holdout.res$residuals), 5),
    RMSE = RMSE(pred=holdout.res$predicted, obs=holdout.res$actual),
    MAE = MAE(pred=holdout.res$predicted, obs=holdout.res$actual)
)
```
